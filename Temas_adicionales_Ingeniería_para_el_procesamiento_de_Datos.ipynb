{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JOSELUISMILLA/NET8APIDEMO/blob/main/Temas_adicionales_Ingenier%C3%ADa_para_el_procesamiento_de_Datos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from datetime import datetime, timedelta\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import sqlite3\n",
        "\n",
        "# Función para hacer scraping de descripciones basadas en palabras clave, categoría y fecha\n",
        "def get_search_descriptions(keyword, category, date, num_results=5):\n",
        "    # Construir la URL de búsqueda utilizando el keyword, categoría y fecha\n",
        "    search_query = f\"{keyword} {category} {date.strftime('%B %Y')}\"\n",
        "    search_url = f\"https://www.google.com/search?q={search_query}\"\n",
        "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
        "    response = requests.get(search_url, headers=headers)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Extraer los resultados\n",
        "    descriptions = []\n",
        "    for result in soup.select('div.BNeawe.s3v9rd.AP7Wnd'):\n",
        "        descriptions.append(result.text)\n",
        "        if len(descriptions) >= num_results:\n",
        "            break\n",
        "    return descriptions if descriptions else [\"No relevant results found\"]\n",
        "\n",
        "# Simulación de datos numéricos y categóricos\n",
        "num_rows = 1000\n",
        "np.random.seed(42)\n",
        "\n",
        "# ID de transacción\n",
        "trans_ids = range(1, num_rows+1)\n",
        "\n",
        "# Categorías (simulando sectores de productos)\n",
        "categories = np.random.choice([\n",
        "    'Electronics', 'Clothing', 'Food', 'Books', 'Sports', 'Furniture',\n",
        "    'Beauty & Personal Care', 'Health & Wellness', 'Automotive', 'Home Appliances',\n",
        "    'Toys & Games', 'Jewelry', 'Footwear', 'Office Supplies', 'Pet Supplies',\n",
        "    'Baby Products', 'Musical Instruments', 'Gardening', 'Tools & Hardware',\n",
        "    'Kitchenware', 'Luggage & Travel Accessories', 'Stationery', 'Art Supplies',\n",
        "    'Fitness Equipment', 'Outdoor Gear', 'Cleaning Supplies', 'Craft Supplies',\n",
        "    'Party Supplies', 'Photography Equipment', 'Camping & Hiking', 'Home Decor',\n",
        "    'Software', 'Mobile Accessories', 'Watches', 'Cosmetics', 'Personal Hygiene',\n",
        "    'Safety Equipment', 'Pharmaceuticals', 'Medical Devices', 'Energy & Utilities',\n",
        "    'Building Materials', 'Industrial Supplies', 'Agriculture', 'Financial Services',\n",
        "    'Real Estate', 'Transportation', 'Education Materials', 'Travel & Tourism',\n",
        "    'Insurance', 'Consulting Services', 'Entertainment Media'\n",
        "], num_rows)\n",
        "\n",
        "# Variables numéricas correlacionadas con ruido\n",
        "base_var = np.random.uniform(50, 1000, num_rows)\n",
        "var_num_1 = base_var + np.random.normal(0, 20, num_rows)\n",
        "var_num_2 = 0.5 * base_var + np.random.normal(0, 30, num_rows)\n",
        "var_num_3 = 2 * base_var + np.random.normal(0, 50, num_rows)\n",
        "var_num_4 = base_var + np.random.uniform(-100, 100, num_rows)\n",
        "var_num_5 = 1.5 * base_var + np.random.normal(0, 10, num_rows)\n",
        "var_num_6 = 0.3 * base_var + np.random.normal(0, 5, num_rows)\n",
        "\n",
        "# Monto de la transacción\n",
        "amounts = np.random.uniform(10, 1000, num_rows)\n",
        "\n",
        "# Fechas aleatorias en un rango de 2 años\n",
        "start_date = datetime.now() - timedelta(days=730)\n",
        "dates = [start_date + timedelta(days=random.randint(0, 730)) for _ in range(num_rows)]\n",
        "\n",
        "# Variables categóricas adicionales\n",
        "cat_1 = np.random.choice(['Low', 'Medium', 'High'], num_rows)\n",
        "cat_2 = np.random.choice(['A', 'B', 'C', 'D'], num_rows)\n",
        "cat_3 = np.random.choice(['North', 'South', 'East', 'West'], num_rows)\n",
        "cat_4 = np.random.choice(['Urban', 'Rural'], num_rows)\n",
        "cat_5 = np.random.choice(['Single', 'Married', 'Divorced', 'Widowed'], num_rows)\n",
        "cat_6 = np.random.choice(['Red', 'Blue', 'Green', 'Yellow'], num_rows)\n",
        "\n",
        "# Creación del DataFrame con columnas numéricas correlacionadas y categóricas\n",
        "df = pd.DataFrame({\n",
        "    'transaction_id': trans_ids,\n",
        "    'category': categories,\n",
        "    'amount': amounts,\n",
        "    'date': dates,\n",
        "    'var_num_1': var_num_1,\n",
        "    'var_num_2': var_num_2,\n",
        "    'var_num_3': var_num_3,\n",
        "    'var_num_4': var_num_4,\n",
        "    'var_num_5': var_num_5,\n",
        "    'var_num_6': var_num_6,\n",
        "    'cat_1': cat_1,\n",
        "    'cat_2': cat_2,\n",
        "    'cat_3': cat_3,\n",
        "    'cat_4': cat_4,\n",
        "    'cat_5': cat_5,\n",
        "    'cat_6': cat_6\n",
        "})\n",
        "\n",
        "# Palabra clave para el scraping (puedes cambiarla según el caso)\n",
        "keyword = 'trends'\n",
        "\n",
        "# Recopilar descripciones con scraping, utilizando categorías y fechas\n",
        "all_descriptions = []\n",
        "for i in range(num_rows):\n",
        "    category = df['category'][i]\n",
        "    date = df['date'][i]\n",
        "    descriptions = get_search_descriptions(keyword, category, date)\n",
        "    all_descriptions.append(descriptions[0])  # Usar el primer resultado obtenido\n",
        "\n",
        "# Añadir las descripciones al dataframe\n",
        "df['description'] = all_descriptions\n",
        "\n",
        "print(\"Datos generados con correlación, ruido y scraping utilizando categorías y fechas:\")\n",
        "print(df.head())\n",
        "\n",
        "# Conectar a SQLite y usar credenciales\n",
        "conn = sqlite3.connect('transactions_secure.db')\n",
        "c = conn.cursor()\n",
        "\n",
        "# Crear una tabla con credenciales de usuario\n",
        "c.execute('''CREATE TABLE IF NOT EXISTS credentials (\n",
        "                user TEXT NOT NULL,\n",
        "                password TEXT NOT NULL\n",
        "            )''')\n",
        "\n",
        "# Insertar credenciales en la tabla\n",
        "c.execute('''INSERT INTO credentials (user, password)\n",
        "             VALUES ('useradm', '123password')''')\n",
        "\n",
        "# Guardar los datos generados en la base de datos SQLite\n",
        "df.to_sql('transactions', conn, if_exists='replace', index=False)\n",
        "\n",
        "# Cerrar conexión\n",
        "conn.commit()\n",
        "conn.close()\n",
        "\n",
        "print(\"Datos y credenciales guardados en la base de datos SQLite.\")"
      ],
      "metadata": {
        "id": "Z0k2puak_Z14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29d172a2-85e7-4a04-f762-cd9ee92e7ef9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Datos generados con correlación, ruido y scraping utilizando categorías y fechas:\n",
            "   transaction_id               category      amount  \\\n",
            "0               1        Medical Devices  379.784468   \n",
            "1               2  Photography Equipment  704.927728   \n",
            "2               3           Pet Supplies  837.364381   \n",
            "3               4            Agriculture  691.350945   \n",
            "4               5      Health & Wellness  517.456801   \n",
            "\n",
            "                        date   var_num_1   var_num_2    var_num_3   var_num_4  \\\n",
            "0 2024-03-27 23:37:11.383352  660.565577  314.692698  1390.118993  709.702958   \n",
            "1 2023-06-26 23:37:11.383352  497.446535  241.516200  1009.457006  477.335269   \n",
            "2 2023-06-10 23:37:11.383352  581.854517  261.192864  1292.889390  558.548005   \n",
            "3 2022-12-20 23:37:11.383352  719.224405  361.064043  1539.277331  736.427564   \n",
            "4 2024-08-09 23:37:11.383352  555.448805  297.543413  1227.117698  498.152656   \n",
            "\n",
            "     var_num_5   var_num_6   cat_1 cat_2  cat_3  cat_4     cat_5   cat_6  \\\n",
            "0   999.377502  193.846033  Medium     B   East  Urban    Single    Blue   \n",
            "1   765.002678  149.938602    High     A  South  Rural  Divorced   Green   \n",
            "2   915.398121  182.313782    High     A   West  Urban  Divorced  Yellow   \n",
            "3  1129.521205  220.237043  Medium     B   East  Rural   Married     Red   \n",
            "4   856.663989  182.388363    High     B   West  Urban   Widowed  Yellow   \n",
            "\n",
            "                                         description  \n",
            "0  Apr 18, 2024 · The healthcare sectors expected...  \n",
            "1  Aug 13, 2023 · The top trends include AI-enhan...  \n",
            "2  Harvey's offers raw food premixes, to which co...  \n",
            "3  Dec 16, 2022 · The Livestock, Dairy, and Poult...  \n",
            "4  Jan 16, 2024 · Seven areas of growth in the we...  \n",
            "Datos y credenciales guardados en la base de datos SQLite.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Asignar nuevos nombres a las columnas\n",
        "df.columns = [\n",
        "    'ID de Transacción', 'Categoría de Producto', 'Monto de la Transacción', 'Fecha de Transacción',\n",
        "    'Costo de Producción', 'Impuesto Aplicado', 'Precio de Lista', 'Descuento Aplicado',\n",
        "    'Margen de Ganancia', 'Costo de Envío', 'Nivel de Calidad', 'Tamaño de la Empresa',\n",
        "    'Región Geográfica', 'Ubicación', 'Estado Civil del Cliente', 'Color Preferido', 'Descripción de la Transacción'\n",
        "]\n",
        "\n",
        "# Mostrar las primeras filas del DataFrame con los nuevos nombres\n",
        "print(df.head())\n",
        "\n",
        "# Convertir el DataFrame a un archivo Excel\n",
        "df.to_excel('transactions_data.xlsx', index=False)\n",
        "\n",
        "print(\"DataFrame guardado como transactions_data.xlsx\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fsdees7FC-oJ",
        "outputId": "cd23b3f7-8bf9-4e6b-a7ad-afe20887ddbe"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ID de Transacción  Categoría de Producto  Monto de la Transacción  \\\n",
            "0                  1        Medical Devices               379.784468   \n",
            "1                  2  Photography Equipment               704.927728   \n",
            "2                  3           Pet Supplies               837.364381   \n",
            "3                  4            Agriculture               691.350945   \n",
            "4                  5      Health & Wellness               517.456801   \n",
            "\n",
            "        Fecha de Transacción  Costo de Producción  Impuesto Aplicado  \\\n",
            "0 2024-03-27 23:37:11.383352           660.565577         314.692698   \n",
            "1 2023-06-26 23:37:11.383352           497.446535         241.516200   \n",
            "2 2023-06-10 23:37:11.383352           581.854517         261.192864   \n",
            "3 2022-12-20 23:37:11.383352           719.224405         361.064043   \n",
            "4 2024-08-09 23:37:11.383352           555.448805         297.543413   \n",
            "\n",
            "   Precio de Lista  Descuento Aplicado  Margen de Ganancia  Costo de Envío  \\\n",
            "0      1390.118993          709.702958          999.377502      193.846033   \n",
            "1      1009.457006          477.335269          765.002678      149.938602   \n",
            "2      1292.889390          558.548005          915.398121      182.313782   \n",
            "3      1539.277331          736.427564         1129.521205      220.237043   \n",
            "4      1227.117698          498.152656          856.663989      182.388363   \n",
            "\n",
            "  Nivel de Calidad Tamaño de la Empresa Región Geográfica Ubicación  \\\n",
            "0           Medium                    B              East     Urban   \n",
            "1             High                    A             South     Rural   \n",
            "2             High                    A              West     Urban   \n",
            "3           Medium                    B              East     Rural   \n",
            "4             High                    B              West     Urban   \n",
            "\n",
            "  Estado Civil del Cliente Color Preferido  \\\n",
            "0                   Single            Blue   \n",
            "1                 Divorced           Green   \n",
            "2                 Divorced          Yellow   \n",
            "3                  Married             Red   \n",
            "4                  Widowed          Yellow   \n",
            "\n",
            "                       Descripción de la Transacción  \n",
            "0  Apr 18, 2024 · The healthcare sectors expected...  \n",
            "1  Aug 13, 2023 · The top trends include AI-enhan...  \n",
            "2  Harvey's offers raw food premixes, to which co...  \n",
            "3  Dec 16, 2022 · The Livestock, Dairy, and Poult...  \n",
            "4  Jan 16, 2024 · Seven areas of growth in the we...  \n",
            "DataFrame guardado como transactions_data.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "import pandas as pd\n",
        "\n",
        "# Cargar el archivo Excel en un DataFrame\n",
        "df_loaded = pd.read_excel('transactions_data.xlsx')\n",
        "\n",
        "# Conectar a SQLite y usar credenciales\n",
        "conn = sqlite3.connect('transactions_secure.db')\n",
        "c = conn.cursor()\n",
        "\n",
        "# Crear una tabla con credenciales de usuario si no existe\n",
        "c.execute('''CREATE TABLE IF NOT EXISTS credentials (\n",
        "                user TEXT NOT NULL,\n",
        "                password TEXT NOT NULL\n",
        "            )''')\n",
        "\n",
        "# Insertar credenciales (si aún no existen)\n",
        "c.execute('''INSERT INTO credentials (user, password)\n",
        "             VALUES ('useradm', '123password')''')\n",
        "\n",
        "# Guardar el DataFrame cargado en la base de datos SQLite\n",
        "df_loaded.to_sql('transactions', conn, if_exists='replace', index=False)\n",
        "\n",
        "# Cerrar conexión\n",
        "conn.commit()\n",
        "conn.close()\n",
        "\n",
        "print(\"DataFrame cargado de Excel y guardado en la base de datos SQLite con credenciales.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UupPKXoyEBrB",
        "outputId": "436c3cd4-e5fd-4369-ce94-0ff7d9fe882d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame cargado de Excel y guardado en la base de datos SQLite con credenciales.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DsduQ9Z8GAlX",
        "outputId": "6878609e-ecf1-462f-bc66-60fe8c6fdeb8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.3.tar.gz (317.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.3-py2.py3-none-any.whl size=317840625 sha256=1fe70a85f71c64cf08345590d9b20b7271d35f708f2986eddbd3038d761ad9a4\n",
            "  Stored in directory: /root/.cache/pip/wheels/1b/3a/92/28b93e2fbfdbb07509ca4d6f50c5e407f48dce4ddbda69a4ab\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O sqlite-jdbc.jar https://repo1.maven.org/maven2/org/xerial/sqlite-jdbc/3.34.0/sqlite-jdbc-3.34.0.jar"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X3S_mjwMGe7x",
        "outputId": "e5ed3f0f-0c29-4dc7-c655-8299e53262a4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-09-29 00:02:11--  https://repo1.maven.org/maven2/org/xerial/sqlite-jdbc/3.34.0/sqlite-jdbc-3.34.0.jar\n",
            "Resolving repo1.maven.org (repo1.maven.org)... 199.232.192.209, 199.232.196.209, 2a04:4e42:4c::209, ...\n",
            "Connecting to repo1.maven.org (repo1.maven.org)|199.232.192.209|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7296329 (7.0M) [application/java-archive]\n",
            "Saving to: ‘sqlite-jdbc.jar’\n",
            "\n",
            "\rsqlite-jdbc.jar       0%[                    ]       0  --.-KB/s               \rsqlite-jdbc.jar     100%[===================>]   6.96M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2024-09-29 00:02:12 (52.1 MB/s) - ‘sqlite-jdbc.jar’ saved [7296329/7296329]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install xlsxwriter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XSD6yXqUNOZm",
        "outputId": "7a799c8a-6c6a-48b2-a35d-81b947a08dc3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting xlsxwriter\n",
            "  Downloading XlsxWriter-3.2.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Downloading XlsxWriter-3.2.0-py3-none-any.whl (159 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.9/159.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xlsxwriter\n",
            "Successfully installed xlsxwriter-3.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install reportlab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olWxZP0pPEue",
        "outputId": "d69a4fa5-77a9-4479-d78b-7d6d30596307"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting reportlab\n",
            "  Downloading reportlab-4.2.4-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from reportlab) (10.4.0)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from reportlab) (5.2.0)\n",
            "Downloading reportlab-4.2.4-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: reportlab\n",
            "Successfully installed reportlab-4.2.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar las bibliotecas necesarias\n",
        "import time\n",
        "import sqlite3\n",
        "import pandas as pd\n",
        "import os\n",
        "import logging\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import (\n",
        "    StringIndexer,\n",
        "    VectorAssembler,\n",
        "    StandardScaler,\n",
        "    Tokenizer,\n",
        "    StopWordsRemover,\n",
        "    HashingTF,\n",
        "    IDF,\n",
        "    Word2Vec,\n",
        "    PCA\n",
        ")\n",
        "from pyspark.ml.regression import (\n",
        "    LinearRegression,\n",
        "    DecisionTreeRegressor,\n",
        "    RandomForestRegressor,\n",
        "    GBTRegressor\n",
        ")\n",
        "from pyspark.ml.classification import (\n",
        "    LogisticRegression,\n",
        "    DecisionTreeClassifier,\n",
        "    RandomForestClassifier,\n",
        "    GBTClassifier\n",
        ")\n",
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.ml.evaluation import (\n",
        "    RegressionEvaluator,\n",
        "    BinaryClassificationEvaluator,\n",
        "    ClusteringEvaluator\n",
        ")\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from reportlab.lib.pagesizes import letter\n",
        "from reportlab.pdfgen import canvas\n",
        "\n",
        "# Configuración de logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# Crear un directorio para guardar los modelos y resultados si no existe\n",
        "model_save_path = \"./models\"\n",
        "results_save_path = \"./results\"\n",
        "plots_save_path = os.path.join(results_save_path, \"plots\")\n",
        "os.makedirs(model_save_path, exist_ok=True)\n",
        "os.makedirs(results_save_path, exist_ok=True)\n",
        "os.makedirs(plots_save_path, exist_ok=True)\n",
        "\n",
        "# Inicializar un diccionario para almacenar los tiempos de ejecución\n",
        "times = {}\n",
        "\n",
        "def iniciar_sesion_spark(jdbc_driver_path, app_name=\"PySpark SQLite Integration\"):\n",
        "    \"\"\"Inicializa y retorna una sesión de Spark.\"\"\"\n",
        "    try:\n",
        "        spark = SparkSession.builder \\\n",
        "            .appName(app_name) \\\n",
        "            .config(\"spark.jars\", jdbc_driver_path) \\\n",
        "            .getOrCreate()\n",
        "        logging.info(\"Sesión de Spark iniciada.\")\n",
        "        return spark\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error al iniciar la sesión de Spark: {e}\")\n",
        "        raise\n",
        "\n",
        "def cargar_datos_spark(spark, jdbc_url, dbtable, driver):\n",
        "    \"\"\"Carga datos desde SQLite a un DataFrame de Spark.\"\"\"\n",
        "    try:\n",
        "        start_time = time.time()\n",
        "        df = spark.read.format(\"jdbc\") \\\n",
        "            .option(\"url\", jdbc_url) \\\n",
        "            .option(\"dbtable\", dbtable) \\\n",
        "            .option(\"driver\", driver) \\\n",
        "            .load()\n",
        "        end_time = time.time()\n",
        "        duration = end_time - start_time\n",
        "        logging.info(f\"Datos cargados en PySpark en {duration:.2f} segundos.\")\n",
        "        return df, duration\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error al cargar datos desde SQLite: {e}\")\n",
        "        raise\n",
        "\n",
        "def exploracion_y_limpieza(df):\n",
        "    \"\"\"Realiza exploración y limpieza de los datos.\"\"\"\n",
        "    try:\n",
        "        start_time = time.time()\n",
        "        # Registrar el DataFrame como una vista temporal para consultas SQL\n",
        "        df.createOrReplaceTempView(\"transactions\")\n",
        "\n",
        "        # Consulta SQL avanzada: filtrar transacciones con monto > 500\n",
        "        filtered_df = df.sparkSession.sql(\"\"\"\n",
        "            SELECT\n",
        "                `ID de Transacción`,\n",
        "                `Categoría de Producto`,\n",
        "                `Monto de la Transacción`,\n",
        "                `Fecha de Transacción`,\n",
        "                `Costo de Producción`,\n",
        "                `Impuesto Aplicado`,\n",
        "                `Precio de Lista`,\n",
        "                `Descuento Aplicado`,\n",
        "                `Margen de Ganancia`,\n",
        "                `Costo de Envío`,\n",
        "                `Nivel de Calidad`,\n",
        "                `Tamaño de la Empresa`,\n",
        "                `Región Geográfica`,\n",
        "                `Ubicación`,\n",
        "                `Estado Civil del Cliente`,\n",
        "                `Color Preferido`,\n",
        "                `Descripción de la Transacción`\n",
        "            FROM transactions\n",
        "            WHERE `Monto de la Transacción` > 500\n",
        "        \"\"\")\n",
        "\n",
        "        # Agrupación: promedio del monto de transacción por categoría de producto\n",
        "        aggregated_df = filtered_df.groupBy(\"Categoría de Producto\").agg(\n",
        "            F.avg(\"Monto de la Transacción\").alias(\"Avg_Monto_Trans\")\n",
        "        )\n",
        "\n",
        "        end_time = time.time()\n",
        "        duration = end_time - start_time\n",
        "        logging.info(f\"Exploración y limpieza completadas en {duration:.2f} segundos.\")\n",
        "        return filtered_df, aggregated_df, duration\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error en exploración y limpieza de datos: {e}\")\n",
        "        raise\n",
        "\n",
        "def preprocesamiento_datos(df, categorical_columns, numerical_columns):\n",
        "    \"\"\"Preprocesa los datos para el modelo.\"\"\"\n",
        "    try:\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Convertir 'Fecha de Transacción' a tipo timestamp\n",
        "        df = df.withColumn(\n",
        "            \"Fecha de Transacción\",\n",
        "            F.to_timestamp(\"Fecha de Transacción\", \"yyyy-MM-dd HH:mm:ss.SSSSSS\")\n",
        "        )\n",
        "\n",
        "        # Indexar las columnas categóricas\n",
        "        indexers = [\n",
        "            StringIndexer(inputCol=col, outputCol=f\"{col}_Index\", handleInvalid='keep').fit(df)\n",
        "            for col in categorical_columns\n",
        "        ]\n",
        "\n",
        "        # Ensamblar las características en un vector\n",
        "        assembler = VectorAssembler(\n",
        "            inputCols=[f\"{col}_Index\" for col in categorical_columns] + numerical_columns,\n",
        "            outputCol=\"features_unscaled\"\n",
        "        )\n",
        "\n",
        "        # Escalar las características\n",
        "        scaler = StandardScaler(\n",
        "            inputCol=\"features_unscaled\",\n",
        "            outputCol=\"features\",\n",
        "            withStd=True,\n",
        "            withMean=False\n",
        "        )\n",
        "\n",
        "        # Crear un pipeline de preprocesamiento\n",
        "        preprocessing_pipeline = Pipeline(stages=indexers + [assembler, scaler])\n",
        "\n",
        "        # Ajustar y transformar los datos\n",
        "        preprocessing_start_time = time.time()\n",
        "        preprocessed_df = preprocessing_pipeline.fit(df).transform(df)\n",
        "        preprocessing_end_time = time.time()\n",
        "        duration = preprocessing_end_time - preprocessing_start_time\n",
        "        logging.info(f\"Preprocesamiento completado en {duration:.2f} segundos.\")\n",
        "\n",
        "        total_duration = preprocessing_end_time - start_time\n",
        "        logging.info(f\"Tiempo total de preprocesamiento: {total_duration:.2f} segundos.\")\n",
        "\n",
        "        return preprocessed_df, duration, total_duration\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error en preprocesamiento de datos: {e}\")\n",
        "        raise\n",
        "\n",
        "def entrenar_modelos_regresion(preprocessed_df, features_col, label_col):\n",
        "    \"\"\"Entrena múltiples modelos de regresión y retorna los modelos y sus tiempos.\"\"\"\n",
        "    modelos = {\n",
        "        \"Regresión Lineal\": LinearRegression(featuresCol=features_col, labelCol=label_col),\n",
        "        \"Árbol de Decisión\": DecisionTreeRegressor(featuresCol=features_col, labelCol=label_col),\n",
        "        \"Random Forest\": RandomForestRegressor(featuresCol=features_col, labelCol=label_col, numTrees=100),\n",
        "        \"Gradient-Boosted Trees\": GBTRegressor(featuresCol=features_col, labelCol=label_col, maxIter=100)\n",
        "    }\n",
        "\n",
        "    modelos_entrenados = {}\n",
        "    tiempos_entrenamiento = {}\n",
        "\n",
        "    # Dividir los datos en entrenamiento y prueba\n",
        "    train, test = preprocessed_df.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "    for nombre, modelo in modelos.items():\n",
        "        start_time = time.time()\n",
        "        modelo_entrenado = modelo.fit(train)\n",
        "        end_time = time.time()\n",
        "        duration = end_time - start_time\n",
        "        modelos_entrenados[nombre] = modelo_entrenado\n",
        "        tiempos_entrenamiento[nombre] = duration\n",
        "        logging.info(f\"Modelo de {nombre} entrenado en {duration:.2f} segundos.\")\n",
        "\n",
        "    return modelos_entrenados, tiempos_entrenamiento, train, test\n",
        "\n",
        "def entrenar_modelos_clasificacion(preprocessed_df, features_col, label_col):\n",
        "    \"\"\"Entrena múltiples modelos de clasificación y retorna los modelos y sus tiempos.\"\"\"\n",
        "    modelos = {\n",
        "        \"Regresión Logística\": LogisticRegression(featuresCol=features_col, labelCol=label_col),\n",
        "        \"Árbol de Decisión\": DecisionTreeClassifier(featuresCol=features_col, labelCol=label_col),\n",
        "        \"Random Forest\": RandomForestClassifier(featuresCol=features_col, labelCol=label_col, numTrees=100),\n",
        "        \"Gradient-Boosted Trees\": GBTClassifier(featuresCol=features_col, labelCol=label_col, maxIter=100)\n",
        "    }\n",
        "\n",
        "    modelos_entrenados = {}\n",
        "    tiempos_entrenamiento = {}\n",
        "\n",
        "    # Dividir los datos en entrenamiento y prueba\n",
        "    train, test = preprocessed_df.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "    for nombre, modelo in modelos.items():\n",
        "        start_time = time.time()\n",
        "        modelo_entrenado = modelo.fit(train)\n",
        "        end_time = time.time()\n",
        "        duration = end_time - start_time\n",
        "        modelos_entrenados[nombre] = modelo_entrenado\n",
        "        tiempos_entrenamiento[nombre] = duration\n",
        "        logging.info(f\"Modelo de {nombre} entrenado en {duration:.2f} segundos.\")\n",
        "\n",
        "    return modelos_entrenados, tiempos_entrenamiento, train, test\n",
        "\n",
        "def entrenar_modelo_clustering(preprocessed_df, features_col, k=3):\n",
        "    \"\"\"Entrena un modelo de K-Means y retorna el modelo y su tiempo.\"\"\"\n",
        "    try:\n",
        "        start_time = time.time()\n",
        "        kmeans = KMeans(featuresCol=features_col, k=k, seed=42)\n",
        "        kmeans_model = kmeans.fit(preprocessed_df)\n",
        "        end_time = time.time()\n",
        "        duration = end_time - start_time\n",
        "        logging.info(f\"Modelo de K-Means entrenado en {duration:.2f} segundos.\")\n",
        "        return kmeans_model, duration\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error al entrenar el modelo de K-Means: {e}\")\n",
        "        raise\n",
        "\n",
        "def análisis_texto(preprocessed_df):\n",
        "    \"\"\"Realiza análisis de texto utilizando TF-IDF y Word2Vec.\"\"\"\n",
        "    try:\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Tokenizar la descripción de la transacción\n",
        "        tokenizer = Tokenizer(inputCol=\"Descripción de la Transacción\", outputCol=\"words_token\")\n",
        "\n",
        "        # Remover stopwords\n",
        "        remover = StopWordsRemover(inputCol=\"words_token\", outputCol=\"words_clean\")\n",
        "\n",
        "        # Aplicar HashingTF para TF-IDF\n",
        "        hashing_tf = HashingTF(inputCol=\"words_clean\", outputCol=\"rawFeatures\", numFeatures=1000)\n",
        "        idf = IDF(inputCol=\"rawFeatures\", outputCol=\"tfidf_features\")\n",
        "\n",
        "        # Aplicar Word2Vec\n",
        "        word2vec = Word2Vec(inputCol=\"words_clean\", outputCol=\"word2vec_features\", vectorSize=100, minCount=1)\n",
        "\n",
        "        # Crear un pipeline de análisis de texto\n",
        "        text_pipeline = Pipeline(stages=[tokenizer, remover, hashing_tf, idf, word2vec])\n",
        "\n",
        "        # Ajustar y transformar los datos\n",
        "        text_start_time = time.time()\n",
        "        text_model = text_pipeline.fit(preprocessed_df)\n",
        "        text_df = text_model.transform(preprocessed_df)\n",
        "        text_end_time = time.time()\n",
        "        duration = text_end_time - text_start_time\n",
        "        logging.info(f\"Análisis de texto completado en {duration:.2f} segundos.\")\n",
        "\n",
        "        total_duration = text_end_time - start_time\n",
        "        logging.info(f\"Tiempo total de análisis de texto: {total_duration:.2f} segundos.\")\n",
        "\n",
        "        return text_df, duration, total_duration\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error en análisis de texto: {e}\")\n",
        "        raise\n",
        "\n",
        "def evaluar_modelos_regresion(modelos, test_df, label_col=\"Precio de Lista\"):\n",
        "    \"\"\"Evalúa los modelos de regresión y retorna los resultados.\"\"\"\n",
        "    evaluador = RegressionEvaluator(labelCol=label_col, predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "    resultados = {}\n",
        "\n",
        "    for nombre, modelo in modelos.items():\n",
        "        predicciones = modelo.transform(test_df)\n",
        "        rmse = evaluador.evaluate(predicciones)\n",
        "        resultados[nombre] = rmse\n",
        "        logging.info(f\"{nombre} RMSE: {rmse:.2f}\")\n",
        "\n",
        "    return resultados\n",
        "\n",
        "def evaluar_modelos_clasificacion(modelos, test_df, label_col=\"label\"):\n",
        "    \"\"\"Evalúa los modelos de clasificación y retorna los resultados.\"\"\"\n",
        "    evaluador = BinaryClassificationEvaluator(labelCol=label_col, rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
        "    resultados = {}\n",
        "\n",
        "    for nombre, modelo in modelos.items():\n",
        "        predicciones = modelo.transform(test_df)\n",
        "        auc_score = evaluador.evaluate(predicciones)\n",
        "        resultados[nombre] = auc_score\n",
        "        logging.info(f\"{nombre} AUC: {auc_score:.2f}\")\n",
        "\n",
        "    return resultados\n",
        "\n",
        "def evaluar_modelo_clustering(modelo, df):\n",
        "    \"\"\"Evalúa el modelo de clustering y retorna el Silhouette Score.\"\"\"\n",
        "    evaluator = ClusteringEvaluator()\n",
        "    predicciones = modelo.transform(df)\n",
        "    silhouette = evaluator.evaluate(predicciones)\n",
        "    logging.info(f\"K-Means Silhouette Score: {silhouette:.2f}\")\n",
        "    return silhouette\n",
        "\n",
        "def ajuste_hiperparametros_clasificacion(modelo, train_df, evaluador, param_grid, num_folds=3):\n",
        "    \"\"\"Realiza el ajuste de hiperparámetros usando CrossValidator.\"\"\"\n",
        "    try:\n",
        "        start_time = time.time()\n",
        "        crossval = CrossValidator(\n",
        "            estimator=modelo,\n",
        "            estimatorParamMaps=param_grid,\n",
        "            evaluator=evaluador,\n",
        "            numFolds=num_folds,\n",
        "            parallelism=2  # Ajustar según los recursos disponibles\n",
        "        )\n",
        "        cv_model = crossval.fit(train_df)\n",
        "        end_time = time.time()\n",
        "        duration = end_time - start_time\n",
        "        logging.info(f\"CrossValidator completado en {duration:.2f} segundos.\")\n",
        "        return cv_model, duration\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error en ajuste de hiperparámetros: {e}\")\n",
        "        raise\n",
        "\n",
        "def guardar_modelos(modelos_dict, ruta_base):\n",
        "    \"\"\"Guarda los modelos entrenados en el sistema de archivos, sobrescribiendo si existen.\"\"\"\n",
        "    try:\n",
        "        for nombre, modelo in modelos_dict.items():\n",
        "            ruta = os.path.join(ruta_base, f\"{nombre.replace(' ', '_').lower()}_model\")\n",
        "            # Utilizar write().overwrite().save(path) para sobrescribir si existe\n",
        "            modelo.write().overwrite().save(ruta)\n",
        "            logging.info(f\"Modelo de {nombre} guardado en {ruta}\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error al guardar modelos: {e}\")\n",
        "        raise\n",
        "\n",
        "def exportar_resultados_excel(tiempos, regresion_resultados, clasificacion_resultados, clustering_resultado, ruta):\n",
        "    \"\"\"Exporta los resultados y tiempos a un archivo Excel con múltiples hojas.\"\"\"\n",
        "    try:\n",
        "        with pd.ExcelWriter(ruta, engine='xlsxwriter') as writer:\n",
        "            # Hoja de tiempos\n",
        "            tiempos_df = pd.DataFrame(list(tiempos.items()), columns=['Paso', 'Duración (s)'])\n",
        "            tiempos_df.to_excel(writer, sheet_name='Tiempos de Ejecución', index=False)\n",
        "\n",
        "            # Hoja de regresión\n",
        "            regresion_df = pd.DataFrame(list(regresion_resultados.items()), columns=['Modelo', 'RMSE'])\n",
        "            regresion_df.to_excel(writer, sheet_name='Regresión', index=False)\n",
        "\n",
        "            # Hoja de clasificación\n",
        "            clasificacion_df = pd.DataFrame(list(clasificacion_resultados.items()), columns=['Modelo', 'AUC'])\n",
        "            clasificacion_df.to_excel(writer, sheet_name='Clasificación', index=False)\n",
        "\n",
        "            # Hoja de clustering\n",
        "            clustering_df = pd.DataFrame([{'Modelo': 'K-Means', 'Silhouette Score': clustering_resultado}])\n",
        "            clustering_df.to_excel(writer, sheet_name='Clustering', index=False)\n",
        "\n",
        "            # Información adicional o gráficos pueden ser añadidos aquí\n",
        "\n",
        "        logging.info(f\"Resultados exportados a Excel en {ruta}\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error al exportar resultados a Excel: {e}\")\n",
        "        raise\n",
        "\n",
        "def generar_informe_pdf(ruta_pdf, plots_paths, excel_path, summary_text=\"\"):\n",
        "    \"\"\"Genera un informe PDF incluyendo gráficos y resúmenes.\"\"\"\n",
        "    try:\n",
        "        c = canvas.Canvas(ruta_pdf, pagesize=letter)\n",
        "        width, height = letter\n",
        "        c.setFont(\"Helvetica\", 12)\n",
        "\n",
        "        # Título\n",
        "        c.drawString(50, height - 50, \"Informe del Pipeline de Machine Learning\")\n",
        "\n",
        "        y_position = height - 80\n",
        "\n",
        "        # Resumen\n",
        "        c.drawString(50, y_position, \"Resumen:\")\n",
        "        y_position -= 20\n",
        "        text_object = c.beginText(50, y_position)\n",
        "        for line in summary_text.split('\\n'):\n",
        "            text_object.textLine(line)\n",
        "            y_position -= 15\n",
        "        c.drawText(text_object)\n",
        "\n",
        "        # Agregar gráficos\n",
        "        for plot in plots_paths:\n",
        "            if y_position < 200:\n",
        "                c.showPage()\n",
        "                y_position = height - 50\n",
        "            # Ajustar la posición y el tamaño de la imagen según sea necesario\n",
        "            c.drawImage(plot, 50, y_position - 300, width=500, height=300)\n",
        "            y_position -= 320\n",
        "\n",
        "        # Finalizar el PDF\n",
        "        c.save()\n",
        "        logging.info(f\"Informe PDF generado en {ruta_pdf}\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error al generar el informe PDF: {e}\")\n",
        "        raise\n",
        "\n",
        "def metodo_del_codo(preprocessed_df, features_col, max_k=10):\n",
        "    \"\"\"Determina el número óptimo de clusters usando el método del codo.\"\"\"\n",
        "    sse = []\n",
        "    for k in range(2, max_k+1):\n",
        "        kmeans = KMeans(featuresCol=features_col, k=k, seed=42)\n",
        "        model = kmeans.fit(preprocessed_df)\n",
        "        sse.append(model.summary.trainingCost)\n",
        "        logging.info(f\"K={k}, SSE={model.summary.trainingCost}\")\n",
        "\n",
        "    # Graficar el método del codo\n",
        "    plt.figure(figsize=(10,6))\n",
        "    sns.lineplot(x=range(2, max_k+1), y=sse, marker='o')\n",
        "    plt.title('Método del Codo para Determinar k Óptimo')\n",
        "    plt.xlabel('Número de Clusters (k)')\n",
        "    plt.ylabel('Suma de Errores al Cuadrado (SSE)')\n",
        "    plot_path = os.path.join(plots_save_path, \"metodo_del_codo.png\")\n",
        "    plt.savefig(plot_path)\n",
        "    plt.close()\n",
        "    logging.info(\"Gráfico del método del codo guardado.\")\n",
        "\n",
        "    # Determinar el k donde la disminución de SSE se ralentiza\n",
        "    # Aquí podrías implementar lógica para elegir el k óptimo automáticamente\n",
        "    # Por simplicidad, retornamos el k con la mayor diferencia en la pendiente\n",
        "    diffs = [sse[i] - sse[i+1] for i in range(len(sse)-1)]\n",
        "    if diffs:\n",
        "        k_optimo = diffs.index(max(diffs)) + 3  # +3 porque range starts at 2 and diffs index shifted by 1\n",
        "        logging.info(f\"El número óptimo de clusters según el método del codo es: k={k_optimo}\")\n",
        "    else:\n",
        "        k_optimo = 3  # Valor por defecto\n",
        "        logging.info(f\"No se pudo determinar k óptimo. Usando k={k_optimo} por defecto.\")\n",
        "    return k_optimo\n",
        "\n",
        "def main():\n",
        "    \"\"\"Función principal para ejecutar todo el pipeline.\"\"\"\n",
        "    try:\n",
        "        # Iniciar el temporizador total\n",
        "        total_start_time = time.time()\n",
        "\n",
        "        # -----------------------------\n",
        "        # Paso C: Conexión de PySpark con SQLite\n",
        "        # -----------------------------\n",
        "        sqlite_db_path = \"transactions_secure.db\"  # Ruta al archivo de la base de datos SQLite\n",
        "        sqlite_jdbc_driver = \"sqlite-jdbc.jar\"  # Ruta al driver JDBC de SQLite\n",
        "\n",
        "        # Verificar que el driver JDBC exista\n",
        "        if not os.path.exists(sqlite_jdbc_driver):\n",
        "            raise FileNotFoundError(\n",
        "                f\"El driver JDBC de SQLite no se encontró en {sqlite_jdbc_driver}. \"\n",
        "                \"Por favor, descarga y coloca el archivo JAR correspondiente.\"\n",
        "            )\n",
        "\n",
        "        # URL JDBC para SQLite\n",
        "        jdbc_url = f\"jdbc:sqlite:{sqlite_db_path}\"\n",
        "\n",
        "        # Inicializar la sesión de Spark con el driver JDBC\n",
        "        spark = iniciar_sesion_spark(sqlite_jdbc_driver)\n",
        "\n",
        "        # -----------------------------\n",
        "        # Paso D: Cargar datos a PySpark\n",
        "        # -----------------------------\n",
        "        transactions_df, duration = cargar_datos_spark(spark, jdbc_url, \"transactions\", \"org.sqlite.JDBC\")\n",
        "        times['Cargar datos a PySpark'] = duration\n",
        "\n",
        "        # -----------------------------\n",
        "        # Paso E: Exploración y limpieza de los datos con SQL en PySpark\n",
        "        # -----------------------------\n",
        "        filtered_df, aggregated_df, duration = exploracion_y_limpieza(transactions_df)\n",
        "        times['Exploración y limpieza de los datos'] = duration\n",
        "\n",
        "        # Guardar resultados de agregación\n",
        "        aggregated_pd = aggregated_df.toPandas()\n",
        "        aggregated_pd.to_excel(os.path.join(results_save_path, \"agregacion_categoria_producto.xlsx\"), index=False)\n",
        "        logging.info(\"Resultados de agregación guardados en Excel.\")\n",
        "\n",
        "        # -----------------------------\n",
        "        # Paso F: Preprocesamiento de los datos\n",
        "        # -----------------------------\n",
        "        categorical_columns = [\n",
        "            \"Categoría de Producto\", \"Nivel de Calidad\", \"Tamaño de la Empresa\",\n",
        "            \"Región Geográfica\", \"Ubicación\", \"Estado Civil del Cliente\", \"Color Preferido\"\n",
        "        ]\n",
        "\n",
        "        numerical_columns = [\n",
        "            \"Monto de la Transacción\", \"Costo de Producción\", \"Impuesto Aplicado\",\n",
        "            \"Precio de Lista\", \"Descuento Aplicado\", \"Margen de Ganancia\", \"Costo de Envío\"\n",
        "        ]\n",
        "\n",
        "        preprocessed_df, duration, total_duration = preprocesamiento_datos(\n",
        "            filtered_df,\n",
        "            categorical_columns,\n",
        "            numerical_columns\n",
        "        )\n",
        "        times['Preprocesamiento de los datos'] = duration\n",
        "        times['Preprocesamiento total'] = total_duration\n",
        "\n",
        "        # -----------------------------\n",
        "        # Paso G: Aplicación de algoritmos de Machine Learning con PySpark ML\n",
        "        # -----------------------------\n",
        "\n",
        "        # F1: Entrenamiento de modelos de Regresión\n",
        "        modelos_reg, tiempos_reg, train_reg, test_reg = entrenar_modelos_regresion(\n",
        "            preprocessed_df,\n",
        "            \"features\",\n",
        "            \"Precio de Lista\"\n",
        "        )\n",
        "        times.update(tiempos_reg)\n",
        "\n",
        "        # F2: Entrenamiento de modelos de Clasificación\n",
        "        # Crear etiqueta binaria\n",
        "        classification_df = preprocessed_df.withColumn(\n",
        "            \"label\",\n",
        "            F.when(F.col(\"Monto de la Transacción\") > 700, 1).otherwise(0)\n",
        "        ).select(\"features\", \"label\")\n",
        "\n",
        "        modelos_cls, tiempos_cls, train_cls, test_cls = entrenar_modelos_clasificacion(\n",
        "            classification_df,\n",
        "            \"features\",\n",
        "            \"label\"\n",
        "        )\n",
        "        times.update(tiempos_cls)\n",
        "\n",
        "        # F3: Determinar el número óptimo de clusters usando el método del codo\n",
        "        k_optimo = metodo_del_codo(preprocessed_df, \"features\", max_k=10)\n",
        "        # No se midió el tiempo en 'metodo_del_codo', pero puedes hacerlo si lo deseas\n",
        "\n",
        "        # F4: Entrenamiento de modelo de Clustering K-Means con k_optimo\n",
        "        kmeans_model, duration_kmeans = entrenar_modelo_clustering(preprocessed_df, \"features\", k=k_optimo)\n",
        "        times[f'Clustering K-Means k={k_optimo}'] = duration_kmeans\n",
        "\n",
        "        # F5: Análisis de texto\n",
        "        text_df, duration_text, total_duration_text = análisis_texto(preprocessed_df)\n",
        "        times['Análisis de texto'] = duration_text\n",
        "        times['Análisis de texto total'] = total_duration_text\n",
        "\n",
        "        # -----------------------------\n",
        "        # Paso H: Evaluación y ajuste de modelos\n",
        "        # -----------------------------\n",
        "        # H1: Evaluar modelos de Regresión\n",
        "        regresion_resultados = evaluar_modelos_regresion(modelos_reg, test_reg)\n",
        "\n",
        "        # H2: Evaluar modelos de Clasificación\n",
        "        clasificacion_resultados = evaluar_modelos_clasificacion(modelos_cls, test_cls)\n",
        "\n",
        "        # H3: Evaluar modelo de Clustering\n",
        "        clustering_resultado = evaluar_modelo_clustering(kmeans_model, preprocessed_df)\n",
        "\n",
        "        # H4: Ajuste de hiperparámetros para Clasificación Logística\n",
        "        log_reg = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
        "        paramGrid = ParamGridBuilder() \\\n",
        "            .addGrid(log_reg.regParam, [0.01, 0.1, 1.0]) \\\n",
        "            .addGrid(log_reg.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
        "            .build()\n",
        "        cv_model, duration_cv = ajuste_hiperparametros_clasificacion(\n",
        "            log_reg,\n",
        "            train_cls,\n",
        "            BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\"),\n",
        "            paramGrid\n",
        "        )\n",
        "        times['CrossValidator clasificación logística'] = duration_cv\n",
        "\n",
        "        # Obtener el mejor modelo de Clasificación Logística\n",
        "        best_log_reg_model = cv_model.bestModel\n",
        "        logging.info(f\"Mejores parámetros del modelo de Clasificación Logística: regParam={best_log_reg_model._java_obj.getRegParam()}, elasticNetParam={best_log_reg_model._java_obj.getElasticNetParam()}\")\n",
        "\n",
        "        # Actualizar resultados de clasificación con el mejor modelo\n",
        "        predicciones_best = best_log_reg_model.transform(test_cls)\n",
        "        evaluador_cls = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
        "        auc_best = evaluador_cls.evaluate(predicciones_best)\n",
        "        clasificacion_resultados['Clasificación Logística Optimizada'] = auc_best\n",
        "        logging.info(f\"Clasificación Logística Optimizada AUC: {auc_best:.2f}\")\n",
        "\n",
        "        # Agregar el tiempo total de evaluación y ajuste\n",
        "        duration_evaluacion = time.time() - total_start_time\n",
        "        times['Evaluación y ajuste de modelos'] = duration_evaluacion\n",
        "\n",
        "        # -----------------------------\n",
        "        # Paso I: Visualización de resultados\n",
        "        # -----------------------------\n",
        "        start_time_visual = time.time()\n",
        "        plots_paths = []\n",
        "\n",
        "        # H1: Visualizar predicciones de Regresión Lineal\n",
        "        lr_model = modelos_reg.get(\"Regresión Lineal\")\n",
        "        if lr_model:\n",
        "            predictions_reg = lr_model.transform(test_reg)\n",
        "            regression_results = predictions_reg.select(\"Precio de Lista\", \"prediction\").toPandas()\n",
        "            plt.figure(figsize=(10,6))\n",
        "            sns.scatterplot(x='Precio de Lista', y='prediction', data=regression_results, alpha=0.5)\n",
        "            plt.title('Regresión Lineal: Actual vs Predicción')\n",
        "            plt.xlabel('Precio de Lista Real')\n",
        "            plt.ylabel('Precio de Lista Predicho')\n",
        "            plot_path = os.path.join(plots_save_path, \"regresion_lineal_actual_vs_prediccion.png\")\n",
        "            plt.savefig(plot_path)\n",
        "            plt.close()\n",
        "            logging.info(\"Visualización de Regresión Lineal guardada.\")\n",
        "            plots_paths.append(plot_path)\n",
        "\n",
        "        # H2: Visualizar curva ROC para Clasificación Logística\n",
        "        if best_log_reg_model:\n",
        "            cls_results = predicciones_best.select(\"label\", \"probability\").toPandas()\n",
        "            cls_results['prob'] = cls_results['probability'].apply(lambda x: x[1])\n",
        "\n",
        "            fpr, tpr, thresholds = roc_curve(cls_results['label'], cls_results['prob'])\n",
        "            roc_auc = auc(fpr, tpr)\n",
        "\n",
        "            plt.figure(figsize=(10,6))\n",
        "            plt.plot(fpr, tpr, label=f'Curva ROC (AUC = {roc_auc:.2f})')\n",
        "            plt.plot([0,1], [0,1], 'k--')\n",
        "            plt.title('Curva ROC - Clasificación Logística Optimizada')\n",
        "            plt.xlabel('Tasa de Falsos Positivos')\n",
        "            plt.ylabel('Tasa de Verdaderos Positivos')\n",
        "            plt.legend(loc='lower right')\n",
        "            plot_path = os.path.join(plots_save_path, \"clasificacion_logistica_roc_curve.png\")\n",
        "            plt.savefig(plot_path)\n",
        "            plt.close()\n",
        "            logging.info(\"Curva ROC de Clasificación Logística guardada.\")\n",
        "            plots_paths.append(plot_path)\n",
        "\n",
        "        # H3: Visualizar resultados de K-Means Clustering\n",
        "        clustering_results = kmeans_model.transform(preprocessed_df).select(\"prediction\").toPandas()\n",
        "        plt.figure(figsize=(10,6))\n",
        "        sns.countplot(x='prediction', data=clustering_results)\n",
        "        plt.title(f'Resultados de K-Means Clustering (k={k_optimo})')\n",
        "        plt.xlabel('Cluster')\n",
        "        plt.ylabel('Número de Transacciones')\n",
        "        plot_path = os.path.join(plots_save_path, \"kmeans_clustering_resultados.png\")\n",
        "        plt.savefig(plot_path)\n",
        "        plt.close()\n",
        "        logging.info(\"Visualización de K-Means Clustering guardada.\")\n",
        "        plots_paths.append(plot_path)\n",
        "\n",
        "        # H4: Visualizar distribución de características TF-IDF\n",
        "        tfidf_sample = text_df.select(\"tfidf_features\").limit(1000).toPandas()\n",
        "        tfidf_sample['tfidf_norm'] = tfidf_sample['tfidf_features'].apply(lambda x: float(x.norm(2)))\n",
        "\n",
        "        plt.figure(figsize=(10,6))\n",
        "        sns.histplot(tfidf_sample['tfidf_norm'], bins=50, kde=True)\n",
        "        plt.title('Distribución de la Norma de Características TF-IDF')\n",
        "        plt.xlabel('Norma TF-IDF')\n",
        "        plt.ylabel('Frecuencia')\n",
        "        plot_path = os.path.join(plots_save_path, \"tfidf_norm_distribution.png\")\n",
        "        plt.savefig(plot_path)\n",
        "        plt.close()\n",
        "        logging.info(\"Distribución de Norma TF-IDF guardada.\")\n",
        "        plots_paths.append(plot_path)\n",
        "\n",
        "        # H5: Visualizar características de Word2Vec usando PCA\n",
        "        pca = PCA(k=2, inputCol=\"word2vec_features\", outputCol=\"pca_features\")\n",
        "        pca_model = pca.fit(text_df)\n",
        "        pca_df = pca_model.transform(text_df).select(\"pca_features\").toPandas()\n",
        "\n",
        "        plt.figure(figsize=(10,6))\n",
        "        sns.scatterplot(\n",
        "            x=pca_df['pca_features'].apply(lambda x: x[0]),\n",
        "            y=pca_df['pca_features'].apply(lambda x: x[1]),\n",
        "            alpha=0.5\n",
        "        )\n",
        "        plt.title('Proyección PCA de Características Word2Vec')\n",
        "        plt.xlabel('PCA 1')\n",
        "        plt.ylabel('PCA 2')\n",
        "        plot_path = os.path.join(plots_save_path, \"word2vec_pca_projection.png\")\n",
        "        plt.savefig(plot_path)\n",
        "        plt.close()\n",
        "        logging.info(\"Proyección PCA de Word2Vec guardada.\")\n",
        "        plots_paths.append(plot_path)\n",
        "\n",
        "        # H6: Visualizar el método del codo\n",
        "        plot_codo = os.path.join(plots_save_path, \"metodo_del_codo.png\")\n",
        "        if os.path.exists(plot_codo):\n",
        "            plots_paths.append(plot_codo)\n",
        "\n",
        "        duration_visual = time.time() - start_time_visual\n",
        "        times['Visualización de resultados'] = duration_visual\n",
        "        logging.info(f\"Visualización completada en {duration_visual:.2f} segundos.\")\n",
        "\n",
        "        # -----------------------------\n",
        "        # Paso J: Cálculo de tiempos y Exportación de Resultados\n",
        "        # -----------------------------\n",
        "        total_end_time = time.time()\n",
        "        tiempos_totales = total_end_time - total_start_time\n",
        "        times['Tiempo total'] = tiempos_totales\n",
        "        logging.info(f\"Tiempo total de ejecución: {tiempos_totales:.2f} segundos.\")\n",
        "\n",
        "        # Exportar resultados a Excel\n",
        "        ruta_excel = os.path.join(results_save_path, \"resultados_pipeline.xlsx\")\n",
        "        exportar_resultados_excel(\n",
        "            times,\n",
        "            regresion_resultados,\n",
        "            clasificacion_resultados,\n",
        "            clustering_resultado,\n",
        "            ruta_excel\n",
        "        )\n",
        "\n",
        "        # -----------------------------\n",
        "        # Paso K: Generar Informe PDF\n",
        "        # -----------------------------\n",
        "        ruta_pdf = os.path.join(results_save_path, \"informe_pipeline.pdf\")\n",
        "        summary_text = f\"\"\"\n",
        "Informe del Pipeline de Machine Learning\n",
        "\n",
        "Tiempo Total de Ejecución: {tiempos_totales:.2f} segundos\n",
        "\n",
        "Modelos de Regresión:\n",
        "\"\"\"\n",
        "        for modelo, rmse in regresion_resultados.items():\n",
        "            summary_text += f\"{modelo}: RMSE = {rmse:.2f}\\n\"\n",
        "\n",
        "        summary_text += \"\\nModelos de Clasificación:\\n\"\n",
        "        for modelo, auc_score in clasificacion_resultados.items():\n",
        "            summary_text += f\"{modelo}: AUC = {auc_score:.2f}\\n\"\n",
        "\n",
        "        summary_text += f\"\\nModelo de Clustering K-Means (k={k_optimo}): Silhouette Score = {clustering_resultado:.2f}\\n\"\n",
        "\n",
        "        generar_informe_pdf(ruta_pdf, plots_paths, ruta_excel, summary_text)\n",
        "\n",
        "        # -----------------------------\n",
        "        # Paso L: Guardar los modelos entrenados\n",
        "        # -----------------------------\n",
        "        modelos_a_guardar = {\n",
        "            \"Regresión Lineal\": modelos_reg.get(\"Regresión Lineal\"),\n",
        "            \"Árbol de Decisión Regresión\": modelos_reg.get(\"Árbol de Decisión\"),\n",
        "            \"Random Forest Regresión\": modelos_reg.get(\"Random Forest\"),\n",
        "            \"Gradient-Boosted Trees Regresión\": modelos_reg.get(\"Gradient-Boosted Trees\"),\n",
        "            \"Regresión Logística\": modelos_cls.get(\"Regresión Logística\"),\n",
        "            \"Árbol de Decisión Clasificación\": modelos_cls.get(\"Árbol de Decisión\"),\n",
        "            \"Random Forest Clasificación\": modelos_cls.get(\"Random Forest\"),\n",
        "            \"Gradient-Boosted Trees Clasificación\": modelos_cls.get(\"Gradient-Boosted Trees\"),\n",
        "            f\"K-Means Clustering k={k_optimo}\": kmeans_model,\n",
        "            \"Clasificación Logística Optimizada\": best_log_reg_model\n",
        "        }\n",
        "        guardar_modelos(modelos_a_guardar, model_save_path)\n",
        "\n",
        "        # -----------------------------\n",
        "        # Paso M: Desconexión y limpieza final\n",
        "        # -----------------------------\n",
        "        spark.stop()\n",
        "        logging.info(\"Sesión de Spark cerrada.\")\n",
        "\n",
        "        # Finalizar el reporte\n",
        "        logging.info(\"Proceso completado exitosamente.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error en el pipeline principal: {e}\")\n",
        "        try:\n",
        "            spark.stop()\n",
        "            logging.info(\"Sesión de Spark cerrada debido a un error.\")\n",
        "        except:\n",
        "            pass\n",
        "        raise\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "Lwc17j0vPBrK"
      },
      "execution_count": 8,
      "outputs": []
    }
  ]
}